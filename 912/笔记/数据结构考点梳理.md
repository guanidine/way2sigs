# 数据结构考点梳理

## 绪论

### 复杂度度量

#### 时间复杂度

* 大$\mathcal{O}$记号

$T(n)\leqslant c\cdot f(n)$，则记为 $T(n)=\mathcal{O}(f(n))$ —— 上界

比较而言，“最坏情况复杂度”是人们最为关注且使用最多的。

* 大$\Omega$记号

$T(n)\geqslant c\cdot g(n)$，则记为 $T(n)=\Omega(g(n))$ —— 下界

* 大$\Theta$记号

$c_1\cdot h(n)\leqslant T(n)\leqslant c_2\cdot h(n)$，则记为 $T(n)=\Theta(h(n))$ —— 同阶

<img src="image\dsa1.png" alt="image-20221008134700762" style="zoom:80%;" />

#### 空间复杂度

除非特别申明，空间复杂度通常并不计入原始输入本身所占用的空间

时间复杂度本身就是空间复杂度的一个天然的上界

### 复杂度分析

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602230905014-1d1e3557-d4b9-4840-a838-b3e55081a6bb.png?x-oss-process=image%2Fresize%2Cw_853%2Climit_0)

###### **必考：复杂度分析与比较**

2017：$f(n)=\mathcal{O}(g(n))$，不一定有 $f(n)=\mathcal{O}(g(n-1))$ —— 正确

> 反例一：$n^n$，$n-1$ 时降一阶
>
> 反例二：$\begin{cases}n,&n是偶数\\n^2,&n是奇数\end{cases}$，$n-1$ 时阶数错开

2018：$T(1)=a$，无论常数 $a$ 有多大，时间复杂度 $T(n)=T(n/2)+\mathcal{O}(1)$ 的解总是 $\mathcal{O}(\log\ n)$ —— 正确

> 本身不难，很显然啊
>
> 涉及到“主定理”：$T(n)=a\cdot T(n/b)+\mathcal{O}(f(n))$
>
> 若 $f(n)=\mathcal{O}(n^{\log_b a-\varepsilon})$，则 $T(n)=\Theta(n^{\log_b a})$
>
> * 例如 $T(n)=2T(n/4)+\mathcal{O}(1)=\Theta(\sqrt{n})$
>
> 若 $f(n)=\Theta(n^{\log_b a-\varepsilon}\cdot \log^k n)$，则 $T(n)=\Theta(n^{\log_b a}\cdot\log^{k+1}n)$
>
> * 例如二分搜索 $T(n)=1T(n/2)+\mathcal{O}(1)=\Theta(\log n)$
> * 例如归并排序 $T(n)=2T(n/2)+\mathcal{O}(n)=\Theta(n\log n)$
> * 例如STL归并排序 $T(n)=2T(n/2)+\mathcal{O}(n\log n)=\Theta(n\log^2 n)$
>
> 若 $f(n)=\Omega(n^{\log_b a-\varepsilon})$，则 $T(n)=\Theta(f(n))$
>
> * 例如 $T(n)=1T(n/2)+\mathcal{O}(n)=\Theta(n)$
>
> 本题也可直接套用主定理做

2019：$n^{\lg\lg\lg n}=\mathcal{O}([\lg n]!)$ —— 正确

> 左边即 ${\rm e}^{\ln\ln\ln n\cdot\ln n}=(\ln\ln n)^{\ln n}\overset{\ln n\overset{\Delta}=M}=(\ln M)^{M}$
>
> *有这么个结论* $(\ln M)^M=\mathcal{O}(M!)$，于是等式得证

2020：$\lg^n n=\Theta(n^{\lg n})$ —— 错误

> 两边取对数（方便起见，直接把 $\lg$ 换成 $\ln$），数学题罢了

#### 级数的复杂度

* 算数级数：与末项平方同阶

  $$T(n)=1+2+\cdots+n=\mathcal{O}(n^2)$$

* 幂方级数：比幂次高出一阶

  $$T_3(n)=1^3+2^3+\cdots+n^3=\mathcal{O}(n^4)$$

* 几何级数（$a>1$）：与末项同阶

  $$T_a(n)=2^0+2^1+\cdots+2^n=\mathcal{O}(2^n)$$

* 收敛级数（*调和级数、对数级数*）：

  $$1+1/2+1/3+\cdots+1/n=\Theta(\log n)$$

  $$\log1+\log2+\log3+\cdots+\log n=\log(n!)=\Theta(n\log n)$$

* 组合：

  $$\sum k\cdot\log k=\mathcal{O}(n^2\log n)$$

  $$\sum k\cdot 2^k=\mathcal{O}(n\cdot 2^n)$$

### 递归分析

* 减而治之、分而治之

* 递归基

以 `sum()` 为例

#### 递归跟踪

<img src="image\dsa2.png" alt="image-20221008142550342" style="zoom: 80%;" />

#### 递归方程

递推关系：$T(n)=T(n-1)+\mathcal{O}(1)=T(n-1)+c_1$

边界条件：$T(0)=\mathcal{O}(1)=c_2$

联立解得：$T(n)=c_1 n+c_2=\mathcal{O}(n)$

## 向量

### 动态空间管理

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1602242239765-5cc59a9a-89d2-4e41-bd92-7f4c03113224.png" alt="image.png" style="zoom: 67%;" />

容量加倍策略，分摊复杂度为 $\mathcal{O}(1)$，装填因子始终不低于 $50\%$

缩容则是反过来，装填因子过小时容量减半

### 向量遍历

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602244502847-c2b729d7-1f4f-4dfc-9b11-7bf439f2216c.png?x-oss-process=image%2Fresize%2Cw_780%2Climit_0)

相比较而言，后一种形式的功能更强，适用范围更广。比如，函数对象的形式支持对向量元素的关联修改，即根据某个（些）元素的数值相应地修改另一元素，而前者要实现这个功能则要繁琐很多。

### 二分查找

#### 二分查找（版本A）

<img src="image\dsa3.png" alt="image-20221008145532690" style="zoom:80%;" />

```c++
template <typename T> static Rank binSearch ( T* A, T const& e, Rank lo, Rank hi ) {
    while ( lo < hi ) {
        Rank mi = ( lo + hi ) >> 1;
        if ( e < A[mi] )	hi = mi;
        else if ( A[mi] < e )	lo = mi + 1;
        else	return mi;
    }
    return -1;
}
```

如果待查的元素在左半区，则需要做一次比较；而如果待查的元素在右半区，则需要做两次比较。因此存在*不均衡性*

平均查找长度为 $\mathcal{O}(1.5\cdot\log_2 n)$

#### Fibonacci查找

改进：按黄金分割比来确定 `mi`

<img src="image\dsa4.png" alt="image-20221008150751940" style="zoom:80%;" />

```c++
#include "..\fibonacci\Fib.h"

template <typename T> static Rank fibSearch ( T* A, T const& e, Rank lo, Rank hi ) {
    Fib fib ( hi - lo ); //用O(log_phi(n = hi - lo)时间创建Fib数列
    while ( lo < hi ) {
        while ( hi - lo < fib.get() )	fib.prev();
        Rank mi = lo + fib.get() - 1;
        if ( e < A[mi] )	hi = mi;
        else if ( A[mi] < e )	lo = mi + 1;
        else	return mi;
    }
    return -1;
}
```

平均查找长度为 $\mathcal{O}(1.44\cdot\log_2 n)$

###### **考过：为什么要考虑使用Fib优化**

Fib查找时以前后黄金分割点为轴点的常系数相同

> 错误。Fib查找是用于做这样的优化的：左边比较一次，右边比较两次，所以要让左边大一点，Fib取前面的黄金分割点就反了。

#### 二分查找（版本B）

改进：无论朝哪个方向深入，都只需做一次元素的大小比较

只有等到区间的宽度已不足2个单元时迭代才会终止

<img src="image\dsa5.png" alt="image-20221008151614319" style="zoom:80%;" />

```c++
template <typename T> static Rank binSearch ( T* A, T const& e, Rank lo, Rank hi ) {
    while ( 1 < hi - lo ) {
        Rank mi = ( lo + hi ) >> 1;
        ( e < A[mi]  ) ? hi = mi : lo = mi;
    }
    return ( e == A[lo] ) ? lo : -1 ;
}
```

最好情况下的效率有所倒退，最坏情况下的效率有所提高，整体性能更趋稳定。

#### 二分查找（版本C）

改进：

1. 当宽度缩短至0（而不是1）时，查找方告终止
2. 每次转入后端分支时，左边界取作 `mi+1` 而不是 `mi`

可以证明，A[0, lo)中的元素皆*不大于*e；A[hi, n)中的元素皆*大于*e。因此，最终的 `A[lo-1]` 即是原向量中*不大于 `e` 的最后一个元素*。

<img src="image\dsa6.png" alt="image-20221008152427796" style="zoom:80%;" />

```c++
template <typename T> static Rank binSearch ( T* A, T const& e, Rank lo, Rank hi ) {
    while ( lo < hi ) {
        Rank mi = ( lo + hi ) >> 1;
        ( e < A[mi] ) ? hi = mi : lo = mi + 1;
    }
    return --lo;
}
```

#### 插值查找

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602397414755-25a037a4-684c-4d60-9f02-80e23c49f062.png?x-oss-process=image%2Fresize%2Cw_832%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602397635852-6bf33e60-606f-42fe-9855-49b04df5cbb1.png?x-oss-process=image%2Fresize%2Cw_837%2Climit_0)

复杂度 $\mathcal{O}(\log\log n)$ 的估算：

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602398050972-f7a30c64-c368-4bf7-b181-3eaafae734b2.png?x-oss-process=image%2Fresize%2Cw_778%2Climit_0)

改进不明显，且易受干扰和“蒙骗”，此外还需要引入乘法、除法运算

实际可行的算法：

* 大规模：插值查找
* 中规模：折半查找
* 小规模：顺序查找

### 排序算法

#### 比较树

CBA（比较式算法，comparison-based algorithm）式算法，最坏情况下的最低执行成本，可由对应的比较树判定

###### **考过：比较树的使用前提 —— “最坏情况下”**

比较树没考，考过“*最坏情况*”这个点：基于CBA的排序算法时间复杂度是 $\Omega(n\log n)$ —— 错误

> 最好情况可能是 $\mathcal{O}(n)$ 的，低于这个下界
>
> 但是教材原文是这么说的，*最坏情况下*CBA式排序算法至少需要 $\Omega(n\log n)$ 时间，相当接近啊，也有可能是回忆版题目写错了

#### 起泡排序

```c++
template <typename T> void Vector<T>::bubbleSort ( Rank lo, Rank hi )
{ while ( !bubble ( lo, hi-- ) ); }

template <typename T> bool Vector<T>::bubble ( Rank lo, Rank hi ) {
    bool sorted = true;
    while ( ++lo < hi )
        if ( _elem[lo - 1] > _elem[lo] ) {
            sorted = false;
            swap ( _elem[lo - 1], _elem[lo] );
        }
    return sorted;
}
```

最好 $\mathcal{O}(n)$，最坏 $\mathcal{O}(n^2)$，稳定排序（若改为 `if ( >= )` 则变为不稳定排序）

一旦经过某趟扫描之后未发现任何逆序的向量元素，即意味着排序任务已经完成，则通过返回标志 `sorted`，以便主算法及时终止。

###### **考过：起泡排序特殊情况**

> 1. 可能任何元素都无需移动
> 2. 可能某元素会一度朝远离其最终位置的方向逆向移动
> 3. 某元素初始位置与最终位置相邻，甚至已处于最终位置，但却要参与 $n-1$ 次交换
> 4. 所有元素都要参与 $n-1$ 次交换
>
> 习题1-3

#### 归并排序

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602412469509-19896d53-240a-46ec-9dd9-57e5cf4bd956.png?x-oss-process=image%2Fresize%2Cw_842%2Climit_0)

基于比较式CBA的算法，如冒泡排序，插入，选择，归并等

求解排序问题的复杂度范围为 $n\log n \sim  n^2$，下界是 $\Theta(n\log n)$   

## 列表

### 从向量到列表

向量 $\mathcal{O}(1)$ 寻址，$\mathcal{O}(n)$ 增删元素 —— “静态存储”策略

列表 $\mathcal{O}(n)$ 寻址，$\mathcal{O}(1)$ 增删元素 —— “动态存储”策略

### ADT

Tsinghua版列表（双向链表，有Header和Trailer两个哨兵）

<img src="image\dsa7.png" alt="image-20221011132841905" style="zoom:80%;" />

### 排序算法

#### 插入排序

当前位置元素往前插入

<img src="image\dsa8.png" alt="image-20221011133634621" style="zoom:80%;" />

最好 $\mathcal{O}(n)$，最坏 $\mathcal{O}(n^2)$，稳定排序

*输入敏感*。已经有序则为 $\mathcal{O}(n)$，完全逆序则为 $\mathcal{O}(n^2)$，等概率下平均 $\mathcal{O}(n^2)$

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602655308768-069f7ea1-bb16-474a-adb9-cdd45021a15c.png?x-oss-process=image%2Fresize%2Cw_802%2Climit_0)

###### **考过：逆序对**

交换序列中任意一对逆序元素，逆序对数必然减少

> 正确。必然是这样的。。。

序列 $(64,\,63,\,\ldots,\,2,\,1)$ 插排，比较次数接近于（）

> 逆序对有 $63+62+\cdots+1=2016$
>
> 注：移动次数呢？总共的移动次数应该是 $\mathcal{O}(n)$ 的

###### **考过：插入排序与选择排序比较**

插入排序相比于选择排序，最好情况复杂度低，为 $\mathcal{O}(n)$；插入排序稳定？（选择排序不稳定吗？选择最大的那个，稳不稳定取决于选的是哪个最大值，选最靠右的就稳定了。。。）；插入排序可以在线。

###### **考过：向量与列表排序区别**

插入排序使用二分查找寻找插入的位置，时间复杂度可降为 $\mathcal{O}(n\log n)$

> 错误。如果是向量，插入时移动需要 $\mathcal{O}(n)$；如果是列表，二分查找并没啥用。时间复杂度并没有提升。

#### 选择排序

无序部分最大者，放入有序部分最前面

<img src="image\dsa9.png" alt="image-20221011134315278" style="zoom:80%;" />

固定 $\Theta(n)$，稳定排序（不好说，看上面）

*相比于起泡排序*，选择排序的*移动次数*从 $\mathcal{O}(n^2)$ 降至 $\mathcal{O}(1)$（这也是为什么我们在列表中讲的是选择排序而非起泡排序），而比较次数为 $\mathcal{O}(n^2)$

借助更高级的数据结构，可将 `selectMax()` 操作复杂度降至 $\mathcal{O}(\log n)$，从而将选择排序复杂度降至 $\mathcal{O}(n\log n)$

#### 循环节

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603286456394-8e1e7cdf-9069-4c50-a4f4-22653a30f069.png?x-oss-process=image%2Fresize%2Cw_1065%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603286515118-638d26f5-78ed-42c5-9309-b4a3d16561e6.png?x-oss-process=image%2Fresize%2Cw_1067%2Climit_0)

###### **考过：插入排序循环节**

插入排序过程中，有序前缀每增加一个元素，序列中的循环节个数即使不增加，亦不致减少。

> 错误。应该是反了？好像也不是，最后有序的序列，每个元素都是一个循环节。
>
> 应该是取出的那一刻有单调性，插入后不好说。

<img src="image\dsa19.png" alt="image-20221015160253141" style="zoom: 67%;" />

#### 归并排序

$\mathcal{O}(n\log n)$ —— 逆序对个数

## 栈与队列

### 栈与递归

#### 函数调用栈

递归算法所需空间，主要取决于递归深度

<img src="image\dsa10.png" alt="image-20221011142038268" style="zoom:80%;" />

###### **考过：调用栈的结构**

程序执行过程中，调用栈中若同时有多帧对应同一函数，则它们必然依次紧邻排列

> 错误。`f(n)`调用`g(n)`，`g(n)`调用`f(n-1)`，这样`f`就被隔开了。

#### 避免递归

* 隐式的入栈和出栈操作会令实际的运行时间增加不少，可以通过显式地模拟调用栈的运转过程来节省空间，提高效率

* 递归（特别是尾递归）转迭代

### 栈混洗

一个队列 $A$，按一定顺序push和pop生成队列 $B$，称 $B$ 为 $A$ 的一个栈混洗（Stack Permutation）

鉴别一个队列是否是栈混洗（充要条件）：

$A=[\ldots,\,i,\,\ldots,\,j,\,\ldots,\,k,\,\ldots]$

则诸如 $[\ldots,\,k,\,\ldots,\,i,\,\ldots,\,j,\,\ldots]$ 这样的序列是禁形，不可能是 $A$ 的栈混洗

#### 卡特兰数

$\displaystyle SP(1)=1,\,SP(n)=\sum_{k=2}^{n-1} SP(k-1)\cdot SP(n-k)$

$SP(n)$ 即卡特兰数 $catalan(n)=\dfrac{(2n)!}{n!(n+1)!}$

$SP(2)=4!/2!/3!=2$

$SP(3)=6!/3!/4!=5$

$SP(6)=12!/6!/7!=132$

$\cdots\ \cdots$

###### **常考：各种乱七八糟的东西与卡特兰数的联系**

真二叉树（叶节点只能出现在最后一层）是 $Catalan(n-1)$，其他都是 $Catalan(n)$

也考过直接计算的，要把卡特兰数的形式记下来。

<img src="image\dsa12.png" alt="image-20221011150755045" style="zoom: 67%;" />

### 括号匹配

自左向右逐个考查各字符，忽略所有非括号字符。

凡遇到左括号，无论属于哪类均统一压入栈S中。

若遇右括号，则弹出栈顶的左括号并与之比对。若二者属于同类，则继续检查下一字符；否则，即可断定表达式不匹配。
当然，栈S提前变空或者表达式扫描过后栈S非空，也意味着不匹配。

（这也就是判定一个序列是否为栈混洗的方法）

<img src="image\dsa11.png" alt="image-20221011144005440" style="zoom:80%;" />

### 逆波兰表达式（RPN）

#### 中缀转后缀

$0!+123+4*(5*6!+7!/8)/9$

加括号

$(((0!)+123)+((4*((5*(6!))+((7!)/8)))/9))$

符号移至右括号后（如果前面一步括号没加多，这一步每一个右括号后都有且只有一个运算符）

$(((0)!123)+((4((5(6)!)*((7)!8)/)+)*9)/)+$

去括号

$0\ !\ 123\ +\ 4\ 5\ 6\ !\ *\ 7\ !\ 8\ /\ +\ *\ 9\ /\ +$

#### 后缀表达式运算

###### **必考：推算RPN中某个运算符是什么**

注意一下： $$1\ 10\ /$$ 的计算为 $1/10$ ，即 $前者\ opnd\ 后者$

**考过：RPN与中缀表达式相比的优势**

中缀表达式转RPN所需成本已经相当于一次常规表达式求值，那么转换的意义何在？

> 通常一个表达式会做很多次表达式求值（如一个循环中），在多次进行同样的表达式运算时，不再需要比较优先级，故而可以加快运算速度。

#### 中缀表达式运算

###### **考过：非法表达式执行evaluate算法后的结果**

模拟“求值”就好，低优先级的运算符入栈前，将栈内优先级更高的运算符出栈

<img src="image\dsa13.png" alt="image-20221011152840631" style="zoom: 80%;" />

## 二叉树及其表示

### 树

半线性数据结构，兼顾高效的查找、插入和删除操作

树：连通无环图、极小连通图、极大无环图

树的边和节点数的关系：**e = n-1**

结点深度：从节点v*到根结点的路径*的边的个数 $depth(v)$，根节点 $depth(v)=0$

结点高度：$max\{所有子结点的高度\}+1$

树的高度：树T中所有节点深度最大值成为该树的高度（注意树的*高度*是由某一叶子节点的*深度*决定的 —— 其实就等于根节点的高度）

**约定：**根节点深度为0，仅含单个节点的树的高度为0，空树的高度为-1

> $height(v)+depth(v)\leqslant height(T)$，当仅当v在最长链上时取等号
>
> p是v的父亲，则 $height(p)\geqslant height(v)+1$

### 多叉树

#### 父节点

本身信息+父节点秩

空间：$\mathcal{O}(n)$

时间：父节点 $\mathcal{O}(1)$，根节点 $\mathcal{O}(n)$ 或 $\mathcal{O}(1)$；孩子节点 $\mathcal{O}(n)$（需要访遍所有节点）

#### 孩子节点

本身信息+孩子节点序列长度+孩子节点序列

空间：$\mathcal{O}(n)$

时间：父节点 $\mathcal{O}(n)$（需要访遍所有节点），根节点 $\mathcal{O}(n)$ 或 $\mathcal{O}(1)$；孩子节点 $\mathcal{O}(1)$

#### 父节点+孩子节点

本身信息+父节点秩+孩子节点序列长度+孩子节点序列

空间：每个节点孩子序列长短不一，平均 $\mathcal{O}(1)$ 个孩子序列长度，但极端情况下空间消耗可能会有 $\mathcal{O}(n^2)$

时间：父节点 $\mathcal{O}(1)$；孩子节点 $\mathcal{O}(1)$

#### 长子+兄弟

<img src="https://cdn.nlark.com/yuque/0/2020/jpeg/1458680/1600858377739-f8cce375-22a5-4a34-b923-7b813df2742c.jpeg" alt="1600858082(1).jpg" style="zoom: 67%;" />

对于度数为 $d$ 的节点，可以在 $\mathcal{O}(d+1)$ 时间遍历其所有孩子

若再设置parent引用，则 parent()接口也仅需 $\mathcal{O}(1)$

> 有序多叉树=二叉树

### 二叉树

每个节点的度数不超过 $2$

* 第 $k$ 层（深度为 $k$）至多 $2^k$ 个节点
* 含有 $n$ 个节点的二叉树，高度为 $h$，则有 $h+1\leqslant n\leqslant 2^{h+1}-1$（单链 - 满二叉树）

* 度数为 $0$ 的节点有 $n_0$ 个，为 $1$ 的有 $n_1$ 个，为 $2$ 的有 $n_2$ 个，则边数 $e=n-1=n_1+2n_2$，叶子节点数即 $n_0=n_2+1$

  可知 $n_0$ 与 $n_1$ 无关，$h=0$ 时 $n_0=1,\,n_2=0$，随后 $n_0$ 与 $n_2$ 同步递增

###### **考过：二叉树高**

关键字 $1,\,2,\,\ldots,\,2016$ 插入初始为空的平衡二叉树中，只有一个根节点的二叉树高度为 $0$，那么最终树高为多少？

> 就是题目比较难懂。。。$1023< 2016< 2048$，树高 $10$

#### 真二叉树

只含 $n_0$ 和 $n_2$ 的二叉树

通过引入 $n_1+2n_0$ 个外部节点，可将任意二叉树转化为真二叉树（这只是逻辑上假想的，如此转化之后，全树自身的复杂度并未实质增加）

### 树的遍历

#### 先序遍历

```cpp
void travPre_R ( BinNodePosi(T) x, VST& visit ) {
    if ( !x ) return;
    visit ( x->data );
    travPre_R ( x->lc, visit );
    travPre_R ( x->rc, visit );
}
```

##### 递归改为迭代

<img src="image\dsa14.png" alt="image-20221013131039388" style="zoom:80%;" />

```cpp
visitAlongLeftBranch {
    while ( x ) {
        visit ( x->data );	// 立即访问节点
        S.push ( x->rc );
        x = x->lc;
    }
}

travPre {
    Stack<BinNodePosi(T)> S;
    while ( true ) {
        visitAlongLeftBranch ( x, visit, S );
        if ( S.empty() )	break;
        x = S.pop();	// 栈序访问右子树
    }
}
```

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600938465877-2bd525db-b2fd-4e8d-9b2b-07bacfe8e779.png?x-oss-process=image%2Fresize%2Cw_841%2Climit_0)

#### 中序遍历

```cpp
void travIn_R ( BinNodePosi(T) x, VST& visit ) {
    if ( !x ) return;
    travIn_R ( x->lc, visit );
    visit ( x->data );
    travIn_R ( x->rc, visit );
}
```

##### 递归改为迭代

<img src="image\dsa15.png" alt="image-20221013132622282" style="zoom:80%;" />

```cpp
visitAlongLeftBranch {
    while ( x ) {
        S.push ( x->rc );
        x = x->lc;
    }
}

travIn {
    Stack<BinNodePosi(T)> S;
    while ( true ) {
        visitAlongLeftBranch ( x, S );
        if ( S.empty() )	break;
        x = S.pop();
        visit( x->data );	// 栈序访问节点
        x = x->rc;	// 栈序访问右子树
    }
}
```

##### 直接后继

找出中序遍历中某个节点的下一个节点

```cpp
template <typename T> BinNodePosi(T) BinNode<T>::succ() {
    BinNodePosi(T) s = this;
    if ( rc ) {	// 若有右孩子，则直接后继必在右子树中，具体地就是
        s = rc;	// 右子树中
        while ( HasLChild ( *s ) ) s = s->lc;	// 最靠左（最小）的节点
    } else {	// 否则，直接后继应是“将当前节点包含于其左子树中的最低祖先”，具体地就是
        while ( IsRChild ( *s ) ) s = s->parent;	// 逆向地沿右向分支，不断朝左上方移劢
        s = s->parent;	// 最后再朝右上方移动一步，即抵达直接后继（如果存在）
    }
    return s;
}
```

##### 优化一：找到直接后继

```cpp
void travIn_I2 ( BinNodePosi(T) x, VST& visit ) {
    Stack<BinNodePosi(T)> S;
    while ( true )
        if ( x ) {
            S.push ( x );
            x = x->lc;
        } else if ( !S.empty() ) {
            x = S.pop();	// 尚未访问的最低祖先节点退栈
            visit ( x->data );
            x = x->rc;
        } else
            break;
}
```

##### 优化二：无需辅助栈

<img src="image\dsa16.png" alt="image-20221013140354777" style="zoom:80%;" />

```cpp
void travIn_I3 ( BinNodePosi(T) x, VST& visit ) {
    bool backtrack = false;	// 前一步是否刚从右子树回溯
    while ( true ) {
        if ( !backtrack && HasLChild ( *x ) ) // 若有左子树且不是刚刚回溯，则
            x = x->lc;	// 深入遍历左子树
        else {	// 否则——无左子树或刚刚回溯（相当于无左子树）
            visit ( x->data );	// 访问该节点
            if ( HasRChild ( *x ) ) {	// 若其右子树非空，则
                x = x->rc;	// 深入右子树继续遍历
                backtrack = false;	// 并关闭回溯标志
            } else {	// 若右子树空，则
                if ( ! ( x = x->succ() ) )	break;	// 回溯（含抵达末节点时的退出返回）
                backtrack = true;	//并设置回溯标志
            }
        }
    }
}
```

空间复杂度为 $\mathcal{O}(1)$，属于就地算法。但反复调用 `succ()` 函数，时间效率会有所倒退。

##### 优化三：无需辅助栈和辅助标志位

```cpp
void travIn_I4 ( BinNodePosi(T) x, VST& visit ) {
    while ( true ) {
        if ( HasLChild ( *x ) ) // 若有左子树，则
            x = x->lc;	// 深入遍历左子树
        else {	// 否则
            visit ( x->data );	// 访问该节点，并
            while ( HasRChild ( *x ) ) {	// 不断在无右分支处
                if ( ! ( x = x->succ() ) )	return;	// 回溯至直接后继节点（含抵达末节点时的退出返回）
                else	visit(x->data);	// 访问新的当前节点
            }
            x = x->rc;	// （直到有右分支处）转向非空的右子树
        }
    }
}
```



#### 后序遍历

```cpp
void travPost_R ( BinNodePosi(T) x, VST& visit ) {
    if ( !x ) return;
    travPost_R ( x->lc, visit );
    travPost_R ( x->rc, visit );
    visit ( x->data );
}
```

##### 递归改为迭代

HLVFL：最高左侧可见叶节点 —— 从左侧看去，未被遮挡的最高叶节点

<img src="image\dsa17.png" alt="image-20221013140559507" style="zoom:80%;" />

```cpp
gotoHLVFL {
    while ( BinNodePosi(T) x = S.top() )
        if ( HasLChild ( *x ) ) {	//尽可能向左
            if ( HasRChild ( *x ) ) S.push ( x->rc );	//若有右孩子，优先入栈
            S.push ( x->lc );	//然后才转至左孩子
        } else	//实不得已
            S.push ( x->rc );	//才向右
    S.pop();	//返回前，弹出栈顶的空节点
}

travPost_I {
    Stack<BinNodePosi(T)> S;
    if ( x )	S.push ( x );	//根节点入栈
    while ( !S.empty() ) {
        if ( S.top() != x->parent )	// 若栈顶非当前节点之父（则必为其右兄），此时需
            gotoHLVFL ( S );	// 在以其右兄为根的子树中，找到HLVFL（相当于递归深入其中）
        x = S.pop(); visit ( x->data );	// 弹出栈顶（即前一节点之后继），并访问之
    }
}
```

#### 层序遍历

```cpp
void BinNode<T>::travLevel ( VST& visit ) { 
    Queue<BinNodePosi(T)> Q;	//辅助队列
    Q.enqueue ( this );	//根节点入队
    while ( !Q.empty() ) {	//在队列再次变空之前，反复迭代
        BinNodePosi(T) x = Q.dequeue(); 
        visit ( x->data );	//取出队首节点并访问之
        if ( HasLChild ( *x ) )	Q.enqueue ( x->lc );	//左孩子入队
        if ( HasRChild ( *x ) )	Q.enqueue ( x->rc );	//右孩子入队
    }
}
```

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600944344952-66b977fa-ddc2-4906-a64c-ccfc331a684c.png?x-oss-process=image%2Fresize%2Cw_801%2Climit_0)

###### **考过：层序遍历辅助队列**

叶节点 $2018$，层序遍历的辅助队列规模不会超过 $2018$

> 正确。这道题也没说是二叉树，直接把叶节点全扔最下岑就好。每岑刚遍历完的时候，队列规模相对达到一次较大值（该岑节点所有孩子节点全部入队，且都没出队），遍历完倒数第二岑的时候，队列中即 $2018$ 个元素。

#### 重构

##### 先序|后序+中序

唯一的拓扑结构

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600956120536-0e6437a7-4fd0-49c2-a664-bb136bce2095.png?x-oss-process=image%2Fresize%2Cw_701%2Climit_0)

##### 先序+后序

产生歧义

但层序遍历是唯一的

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600956804352-abf2d1c5-9cc6-431e-824d-76f2ce654209.png?x-oss-process=image%2Fresize%2Cw_401%2Climit_0)

##### 先序+后序+真二叉树

唯一的拓扑结构 —— 真二叉树每个节点度数必为 $0$ 或 $2$

###### **常考（但可能已经没题出了）：层序遍历唯一**

先序+后序能否确定唯一的层序？答案是可以的

> 毕竟拓扑结构唯一可以出现歧义的情况是某个父节点只有左子树/右子树，而这两种情况的层序遍历恰恰是一样的。

#### 表达式树

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600944512386-c84640b9-75e3-4b3b-b96b-bf5f03483d0b.png?x-oss-process=image%2Fresize%2Cw_827%2Climit_0)

### 完全二叉树

树高 $\lfloor\log n\rfloor$

叶节点 $\lceil\dfrac{n}{2}\rceil$ 或 $\lfloor\dfrac{n+1}{2}\rfloor$

内部节点 $\lfloor\dfrac{n}{2}\rfloor$

* 叶节点不致少于内部节点，但至多多一个（$10=5+5,\,11=6+5$）

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1600951167462-272b6c6d-a8e2-4a76-856d-5d0de0e9b1d9.png?x-oss-process=image%2Fresize%2Cw_818%2Climit_0)

### 二叉编码树

#### PFC编码树

前缀无歧义编码（prefix-free code）：任何两个原始字符所对应的二进制编码串，相互都不得是前缀

PFC编码树：所有字符都对应于叶节点，歧义现象就自然消除了

#### 最优编码树

平均深度 $\displaystyle ald(T)=\sum_{x\in \Sigma}depth(x)/|\Sigma|$

最优编码树 $ald()$ 最小，满足双子性（必为真二叉树）、层次性（叶节点深度之差不超过 $1$） —— 真完全树就是一个最优编码树

#### Huffman编码树

带权平均深度 $wald()$

虽然最优带权编码树不一定仍是完全的，但依然满足某种意义上的层次性：

* 若 $x$ 和 $y$ 出现的概率在所有字符中最低，则必然存在某棵最优带权编码树，使 $x$ 和 $y$ 同处于最底层，且互为兄弟

这就是Huffman算法的由来，其具体实现为

* 为每个字符创建一棵树，即为Huffman森林
* 权重最小的两棵树合并，更新森林
* 继续合并权重最小的两棵树，直到森林中只有一棵树为止

<img src="image\dsa18.png" alt="image-20221013152715625" style="zoom:80%;" />

最优带权编码树也不是唯一的 —— 互换左右子树 $wald()$ 肯定不变，Huffman编码树只是其中的一棵

###### **常考：Huffman树原理、算法**

哈夫曼树交换不同深度的子树，编码成本必然增加

> 错误。三个字符出现频率一致，哈夫曼树必然有一个深度为 $1$，而另外两个深度为 $2$，交换一下显然不影响。
>
> 早年（2018）考过一个PFC树，一样不是必然增加的。

哈夫曼树深度更小的节点的权值可能小于深度更大节点的权值

> 错误。只可能小于下面两节点之和，小于单个节点是不可能的。

$9$ 个字符出现频率为 $0,\,1,\,1,\,2,\,3,\,5,\,8,\,13,\,21$，其哈夫曼编码最大长度是多少

> 排序（已排好），然后从 $0,\,1$ 开始合并森林就好。答案是 $8$，最低的叶节点深度是 $8$（树高为 $8$）。

## 图

### 图的表示

#### 邻接矩阵

时间性能：顶点的动态操作（如插入新的顶点）接口十分耗时。就分摊意义而言，单次操作的耗时 $\mathcal{O}(n)$。

空间性能：实际上可能根本就没有 $\mathcal{O}(n^2)$ 的边（平面图的边数渐进地不超过$\mathcal{O}(n)$，仅与顶点总数大致相当）。此外对于无向图而言，邻接矩阵近一半的单元都是冗余的。

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603115213164-ebc41d73-de5f-4106-89f1-426224a40c1a.png?x-oss-process=image%2Fresize%2Cw_816%2Climit_0)

#### 邻接表

这里仅讲述最基本的邻接表

空间复杂度 $\mathcal{O}(n+e)$

时间复杂度则增加到 $\mathcal{O}(n)$

* 不过与顶点相关的操作，时间性能可能还有提高，如插入顶点可在 $\mathcal{O}(1)$ 时间内完成，不过删除顶点依旧需要遍历所有邻接表，需 $\mathcal{O}(e)$
* 邻接表十分擅长以批量方式处理同一顶点的所有关联边，如遍历从 $v$ 出发的所有，仅需 $\Theta(1+outDegree(v))$ 而不是 $\Theta(n)$

### 图的遍历

#### 遍历树

图的遍历将非线性结构转化为半线性结构，由树边与所有顶点构成一颗支撑树（森林）

图的遍历中的树边、跨边、回边和前向边：

* 树边：DFS森林的实际组成部分
* 前向边：DFS树中一个顶点指向它的一个非子顶点后裔的边
* 回边：DFS树中一个顶点指向它的祖先的边
* 跨边：两个顶点没有祖先与后裔的关系

![img](https://img-blog.csdnimg.cn/20190802211128449.png)

###### **考过：各种边什么情况下会出现**

DFS何时标记前向边，何时标记后向边

> 当前节点 $v$ 进入处于VISITED状态的 $u$，且 $v$ 更早发现（当然也是更晚结束），则为前向边
>
> 当前节点 $v$ 进入处于DISCOVERED状态的 $u$（$v$ 才刚刚发现，并必定更早结束），则为后向边

#### 广度优先搜索

树的层序遍历

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603116628335-5b55ec39-ff1f-4581-a1f3-37e89613fa31.png?x-oss-process=image%2Fresize%2Cw_821%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603116640425-954a9cd4-4dfa-461a-9590-b085de6ba2de.png?x-oss-process=image%2Fresize%2Cw_830%2Climit_0)

时间复杂度 $\mathcal{O}(n+e)$

BFS树中从起点 $s$ 到一个节点 $v$ 的路径，是从 $s$ 到 $v$ 的最短路径

##### 实例

<img src="image\dsa20.png" alt="image-20221016131618602" style="zoom:80%;" />

#### 深度优先搜索

树的先序遍历

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603118093749-0b236967-d21b-4635-a0a3-2b34f88994c3.png?x-oss-process=image%2Fresize%2Cw_797%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603118101680-a9852700-aa5a-4c54-ad2b-81eaff593766.png?x-oss-process=image%2Fresize%2Cw_813%2Climit_0)

时间复杂度 $\mathcal{O}(n+e)$

##### 括号引理

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603133865545-5263a860-4eaa-42ab-84cd-3c13ee5557f9.png?x-oss-process=image%2Fresize%2Cw_827%2Climit_0)

##### 实例

<img src="image\dsa21.png" alt="image-20221016132314338" style="zoom:80%;" />

![image-20221016132414566](image\dsa22.png)

###### **考过：后向边和环路的关系**

有向图经过DFS后有 $k$ 条边被标记为BACKWARD，则它不一定有 $k$ 个环路

> 正确。环路个数$\geqslant$后向边数

<img src="image\dsa35.png" alt="image-20221017163540167" style="zoom:80%;" />

#### 拓扑排序

* 可以拓扑排序的有向图，必定无环

- 任何DAG(有向无环图)，都至少存在一种拓扑排序

##### 零入度算法

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1603544536371-17ad1d27-0dc3-48e9-8543-f66a83a38c75.png" alt="image.png" style="zoom:80%;" />

##### 基于DFS

DFS搜索中各顶点被标记为VISITED的次序，恰好（按逆序）给出了原图的一个拓扑排序

此外，DFS检测环路的特性，也可用于判断输入是否为DAG：发现有回边，即说明不是DAG

![image-20221016134219222](image\dsa23.png)

时间复杂度 $\mathcal{O}(n+e)$

###### **考过：拓扑排序与DFS**

DAG拓扑排序是DFS的（）

> 回溯的逆序。

### 图应用

#### 双连通域分解（BCC）

看PPT和学堂在线

* 遍历到这样一步————指向自己的祖先，则开始回溯，并更新`hca`————最开始的`hca`只是它自己，指向某个祖先后，`hca`就可以指向这个祖先了，并在回溯的时候，回溯路上的各祖先的`hca`也因此可能会更新，`hca`取较小者
* 这意味着以下的子树有边能连向较高的祖先，那么当前的节点去掉后，这个子树就更不可能与整个图断连————确切地说，`hca`比节点自身更小，意味着它不是关节点，去掉不会影响连通性
* 发现了一个关节点后，需要`pop`出一些栈顶的元素，它们构成了一个双联通域。直观上来看，可以一直`pop`到关节点，但事实却并非如此，只能`pop`到关节点下面的那个子节点，因为关节点和子节点在栈内未必是紧挨着的，`pop`过多的话，就有可能把无关的节点也`pop`出来。如PPT所给实例中的CF双连通域，如果`pop`到F，就会把B也`pop`出来。

<img src="image\dsa24.png" alt="image-20221016145405463" style="zoom:80%;" />

#### 优先级搜索（PFS）

时间复杂度：邻接矩阵/邻接表 $\mathcal{O}(n^2)$，优先级队列 $\mathcal{O}((e+n)\times\log n)$

通过指定具体的`PrioUpdater`来编写不同的算法（选择松弛策略）

#### 优先级搜索-最小支撑树（Prim）

Prim算法：最小支撑树总是会采用联接每一割的最短跨越边————如果两割之间还有其他的边，那么采用那一条边将会违背支撑树最小的前提（但不意味着同一割仅能为最小支撑树贡献一条跨越边。

###### **考过：证明Prim算法正确性**

最小支撑树总是会采用联接每一割的最短跨越边

##### 实例

贪心迭代。每次选取一条视野内的最短边即可。

<img src="image\dsa25.png" alt="image-20221016154846464" style="zoom:80%;" />

<img src="image\dsa26.png" alt="image-20221016154916215" style="zoom:80%;" />

时间复杂度 $\mathcal{O}(n^2)$，可借助优先级队列进一步提高。

#### 优先级搜索-最短路径（Dijkstra）

注意和Prim的算法区别（图 $d\rightarrow e$ 对比）

###### **考过：Dijkstra的优化**

Dijkstra用于稠密图时，为何使用多叉堆替换完全二叉堆？多叉堆的分叉数怎么确定？

> 使用完全二叉堆的情况下PFS总体运行时间为 $\mathcal{O}((n+e)\log n)$，稠密图边数很多，效率可能还不如常规版本。采用多叉堆后PFS总体运行时间为 $\mathcal{O}(n+d)\log_d n$，取 $d=2+\dfrac{e}{n}$，对稠密图的复杂度有较大改进————$\mathcal{O}(n^2)$。

优先级搜索：

* 邻接表：更新树外顶点的优先级 $\mathcal{O}(n+e)$，选出新的优先级最高者 $\mathcal{O}(n)$

* 堆：$n\rightarrow\log n$，整体 $\mathcal{O}((n+e)\log n)$，稀疏图效率高，而稠密图反而不如常规版本

* Fibonacci堆：左式堆$\times$新的上滤算法+懒惰合并，取出堆顶元素 $\mathcal{O}(\log n)$，插入/合并 $\mathcal{O}(1)$

  PFS复杂度 $n\cdot\mathcal{O}(\log n)+e\cdot\mathcal{O}(1)=\mathcal{O}(e+n\log n)$

* 多叉堆：堆高降至 $\mathcal{O}(\log_dn)$，而下滤则增至 $d\cdot\log_dn=\dfrac{d}{\ln d}\cdot\ln n$，PFS总体运行时间为 $n\cdot d\log_d n+e\log_dn=(nd+e)\log_d n$

  当 $d\approx 2+\dfrac{e}{n}$ 时，性能最优，$\mathcal{O}(e\cdot\log_{\tfrac{e}{n}+2}n)=\begin{cases}稀疏图\approx n\log_{\tfrac{n}{n}+2}n=\mathcal{O}(n\log n)\\稠密图\approx n^2\log_{\tfrac{n^2}{n}+2}n=\mathcal{O}(n^2)\end{cases}$，改进极大，并可以自适应地实现最优

##### 实例

![image-20221016155341172](image\dsa27.png)

时间复杂度 $\mathcal{O}(n^2)$，可借助优先级队列进一步提高。

## 搜索树

### 查找

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1601015946282-4d80c3da-4d8c-4e39-ac85-77e00c08a418.png" alt="image.png" style="zoom:67%;" />

二分查找树：循关键码访问

任何一颗二叉树是二叉搜索树，当且仅当其中序遍历序列单调非降

查找算法：返回值和`hot`变量，其中`hot`变量始终指向当前节点的父亲，因此在返回的时候，也统一指向“命中节点（也许只是假想的节点NULL）”的父亲

复杂度正比于返回节点 $v$ 的深度，不超过 $\mathcal{O}(h)$

###### **考过：二叉搜索树结构**

二叉搜索树最大的节点没有左/右孩子？

> 左孩子可以有，必定没有右孩子

对于同一个长度为 $n$ 的序列，分别按照递增或递减的顺序构造AVL树，那么“存在正整数 $k$，使 $n=2^k-1$”是“两次构造的AVL树相同”的什么条件？

> 充要条件。当仅当树为满树时，递增和递减顺序构造的AVL树拓扑是一样的————每次插入都可以单旋修复（只插在最左边或最右边）

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1604726832184-1a196128-5a1a-4684-a668-97f99957b15e.png?x-oss-process=image%2Fresize%2Cw_728%2Climit_0)

### 插入与删除

<img src="image\dsa28.png" alt="image-20221017135655356" style="zoom:80%;" />

删除的时候，如果是双分支，则要递归地找待删除节点的直接后继（succ），两者交换直到待删除节点被换到叶节点位置，随后再将其删去。

<img src="image\dsa29.png" alt="image-20221017135742633" style="zoom:80%;" />

复杂度都不会超过全树的高度。

### 平衡二叉搜索树

#### 随机生成与随机组成

随机生成：全排列 $n!$，平均高度为 $\Theta(\log n)$

随机组成：顺序性给定，拓扑联接随机，卡特兰数 $Catalan(n)=\dfrac{(2n)!}{n!(n+1)!}$，平均高度为 $\Theta(\sqrt{n})$

随机生成方式，同一种拓扑结构的二叉树会出现重复，选择随机组成的方式获取的估计值更为可信

#### 理想平衡与适度平衡

树高不可能小于 $\lfloor \log_2 n\rfloor$，叶节点只能出现于最底部两层

然而要想维护这样的二叉树复杂度很高，因此会采用相对宽松的标准，即适度平衡，渐进地不超过 $\mathcal{O}(\log n)$

#### 等价变换

两棵二叉搜索树中序遍历序列相同，则称它们彼此等价

适度平衡二叉树经过精妙的设计，应能做到：

* 单词动态修改操作后，至多只有 $\mathcal{O}(1)$ 处局部不再满足限制条件
* 总可以在 $\mathcal{O}(\log n)$ 时间内，使这 $\mathcal{O}(1)$ 处局部（以至全树）重新平衡

zigzag：顺zig逆zag

<img src="image\dsa30.png" alt="image-20221017142230708" style="zoom: 80%;" />

事实上，经过不超过 $\mathcal{O}(n)$ 次旋转，等价的BST均可以相互转化

### AVL树

#### 平衡因子

$balFac(v)=height(lc(v))-height(rc(v))$

AVL树各节点的平衡因子绝对值不超过 $1$，如何证明AVL树适度平衡？高度为 $h$ 的树，节点至少有 $\Omega(\Phi^n)$，那么反过来树高就是 $\mathcal{O}(\log n)$

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601038757147-8d0ac6ee-0283-4959-9c68-38159614e812.png?x-oss-process=image%2Fresize%2Cw_732%2Climit_0)

#### 节点插入

至多两次旋转，即可恢复平衡

$g,\,p,\,v$：单旋

<img src="image\dsa31.png" alt="image-20221017145930852" style="zoom:80%;" />

$g,\,v,\,p$：双旋

<img src="image\dsa32.png" alt="image-20221017151542231" style="zoom:80%;" />

#### 节点删除

有可能失衡传播至根节点，$\mathcal{O}(\log n)$

$g,\,p,\,v$：单旋

<img src="image\dsa33.png" alt="image-20221017152008463" style="zoom:80%;" />

$g,\,v,\,p$：双旋

<img src="image\dsa34.png" alt="image-20221017152101606" style="zoom:80%;" />

节点插入过程中，经过一次单旋/双旋后，较小的子树高度必然加一，从而一定达成重平衡（指图片中直观意义上的高度。g，p，v子树高度实际上都不变）

而节点删除操作中，每次旋转，较小的子树高度却有可能再减一，从而再次失衡（如这里的R~4~子树，zigzag后高度会变为X节点原来位置的高度，从而将失衡传递了上去。

![image-20221017155629999](C:\Users\Guanidine Beryllium\AppData\Roaming\Typora\typora-user-images\image-20221017155629999.png)

###### **考过：AVL重平衡过程**

AVL树中，（）操作可能会发生两次旋转调整

> 添加/删除节点都有可能

AVL树插入一个节点后可能引起 $\mathcal{O}(\log n)$ 次局部重构

> 错误。插入是 $\mathcal{O}(1)$，删除是 $\mathcal{O}(\log n)$

#### 综合评价

| AVL树                        | 插入                  | 删除                  |
| ---------------------------- | --------------------- | --------------------- |
| 插入/删除瞬间失衡节点数      | $\mathcal{O}(\log n)$ | $1$                   |
| 插入/删除后需要的"3+4"重构数 | $1$                   | $\mathcal{O}(\log n)$ |

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601102862886-63d26612-ce2b-4263-b30d-8eefb10917f5.png?x-oss-process=image%2Fresize%2Cw_822%2Climit_0)

拓扑结构变化量高，意味着无法持久化

### “3+4”重构

算法实际实现：直接拆开来组装————{T~0~，a，T~1~，b，T~2~，c，T~3~}

## 高级搜索树

### Splay树

#### 逐层伸展

最坏情况：单链条结构，可达 $\Omega(n)$

#### 双层伸展

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601106964793-26a900d3-7144-4485-aafb-6d4d2c1666b4.png?x-oss-process=image%2Fresize%2Cw_830%2Climit_0)

双层伸展策略既可将v推至树根，亦可令分支的长度以几何级数（大致折半）的速度收缩。

单次操作分摊复杂度 $\mathcal{O}(\log n)$

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601109857108-a0d74c28-2383-469b-abb4-acaba430353e.png?x-oss-process=image%2Fresize%2Cw_787%2Climit_0)

###### **常考：伸展树操作的效率**

理想随机的伸展树插入操作的分摊时间复杂度为 $\mathcal{O}(\log n)$

> 正确。理想随机的情况下（而不是最坏情况），插入、删除、查找的分摊时间复杂度都是 $\mathcal{O}(\log n)$

如果访问不存在局部性，则伸展树不能保证分摊 $\mathcal{O}(\log n)$ 的性能

> 错误。只是达不到 $\mathcal{O}(1)$ 罢了。考虑局部性后，$m$ 次操作复杂度为 $\mathcal{O}(m\log k+n\log n)$，均摊可达 $\mathcal{O}(1)$

### B-树

分级存储，一次读取一段字节，大大减少I/O次数，适用于对外存的批量访问

#### 多路搜索树

以 $k$ 层为间隔合并，可将二叉搜索树转化为等价的 $2^k$ 路搜索树（内含 $2^k-1$ 个关键码），一次读取一组关键码

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601111129922-fadc7f6a-b3fd-498f-94cb-2a8b337a2bd6.png?x-oss-process=image%2Fresize%2Cw_805%2Climit_0)

#### 多路平衡二叉树

$m$ 阶B-树，即 $m$ 路平衡搜索树

所有外部节点深度均相等，所有内部节点不存有超过 $m-1$ 个关键码（但也不能太少）

> 分支数 $\lceil m/2\rceil\leqslant x\leqslant m$
>
> 关键码数 $\lceil m/2\rceil-1\leqslant n\leqslant m-1$（根节点 $n\geqslant 1$ ———— 用于处理上溢）

故 $m$ 阶B-树也称作 $(\lceil m/2\rceil,\,m)$-树，如 $(2,\,3)$-树，$(3,\,6)$-树，$(7,\,13)$-树

$4$ 阶B-树，各节点的分支数可以是 $2,\,3,\,4$，关键码数可以是 $1,\,2,\,3$ ———— 红黑树

#### 关键码查找

各节点内通常都包含多个关键码，故有可能需要经过（在内存中的）多次比较，才能确定应该转向下一层的哪个节点并继续查找。

查找失败，最终抵达外部节点，即返回NULL。

![image-20221018141315958](image\dsa36.png)

#### 性能分析

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601184948903-b98cc5e5-a054-4a60-9bef-ac39f3ffe475.png?x-oss-process=image%2Fresize%2Cw_768%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601185117368-61839582-750e-4905-8b8b-cf583c2100fb.png?x-oss-process=image%2Fresize%2Cw_791%2Climit_0)

树高 $h=\Theta(\log_mN)$

范围 $\log_m(N+1)\leqslant h\leqslant 1+\log_{\lceil m/2\rceil}(\dfrac{N+1}{2})$

降至BBST的 $\left[\dfrac{1}{\log_2m},\,\dfrac{1}{\log_2m-1}\right]$

###### **常考：B-树树高计算**

搜索7阶B树的第2016个关键字，假设B树根节点在内存中，则最多启动几次I/O

> 代公式计算，树高不超过 $1+\log_4 1008=5$，这是外部节点高度，外部节点在第6层。即内部节点共5层（根节点高度为0）。而根节点已经在内存中了，故I/O需要4次。

2018年zai考一次（2017个关键字，还是4次）

#### 关键码插入

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601190265554-42af30f3-90bf-4f4f-9531-7e7e67ee8aef.png?x-oss-process=image%2Fresize%2Cw_750%2Climit_0)

关键码上升一层，原超级节点分裂

上溢可能会向上传播，乃至到达树根，以至于自成树根，从而导致B-树高度增加

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1601190312684-86664d9d-ab9c-4280-8dd8-f3d20f6673ac.png?x-oss-process=image%2Fresize%2Cw_801%2Climit_0" alt="image.png" style="zoom:94%;" />

#### 关键码删除

可能性一：可以从兄弟节点处转一个关键码过来

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1601192373742-eaa7e8b2-5715-4d41-ad2a-243cde10f57e.png?x-oss-process=image%2Fresize%2Cw_785%2Climit_0" alt="image.png" style="zoom:95%;" />

可能性二：兄弟节点的关键码也不够，父节点关键码拉下来一起合并为一个超级节点

下溢也可能会向上传播，乃至到达树根，以至于将树根并入下一层节点，从而导致B-树高度降低

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1601192403793-183cbd5e-3562-4c14-8c6a-2c4a2dbe07ea.png?x-oss-process=image%2Fresize%2Cw_801%2Climit_0" alt="image.png" style="zoom:93%;" />

### 红黑树

伸展树：实现简便，无需修改节点结构，分摊复杂度低，但最坏单次操作需要 $\Omega(n)$ 时间（确实如此，即便是双层伸展，也有可能出现 $\Omega(n)$，只是分摊复杂度能优化到 $\mathcal{O}(\log n)$），不适用于对稳定性、可靠性要求较高的场合。

AVL树：单次操作速度可以保证，但需要在节点中嵌入平衡因子等标识，而且删除操作后可能需要 $\mathcal{O}(\log n)$ 次旋转才能重平衡，从而导致全树拓扑结构大幅变化。

红黑树：全树拓扑结构的更新仅涉及常数个节点（尽管最坏情况下需要染色 $\Omega(\log n)$ 个节点，但就分摊意义而言仅 $\mathcal{O}(1)$ 个）

进一步放宽适度平衡的条件————任一节点左、右子树高度，相差不得超过两倍

#### 结构

* 树根为黑
* 外部节点为黑
* 其余节点若为红，则其孩子节点必为黑
* 从任一外部节点到根节点的沿途，黑节点数目相等

等价于一棵(2, 4)-树（关键码范围 $1,\,2,\,3$ ），高度 $\mathcal{O}(\log n)$

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601200511498-e65c5fe4-09b0-40ac-b068-fafa3cb7634a.png?x-oss-process=image%2Fresize%2Cw_796%2Climit_0)

红黑树的黑高度不低于高度的一半；反之，高度不超过黑高度的两倍

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601200606860-23011b44-16ad-4ec4-92d6-d200f99c7a70.png?x-oss-process=image%2Fresize%2Cw_811%2Climit_0)

###### **考过：红黑树结构**

红黑树所有节点黑深度与黑高度和相等

> 错误。红节点的黑深度+黑高度比黑节点的黑深度+黑高度少1
>
> 黑深度：从根节点出发通往任一节点，除去根节点本身，沿途经过的黑节点总数。根节点黑高度为0
>
> 黑高度：从任一节点出发，通往外部节点，除去（黑色）外部节点，沿途经过的黑节点总数。外部节点黑高度为0

#### 节点插入算法

节点插入，随后染色为红色，而父节点也可能是红，需要双红修正（RR-1，RR-2）

##### RR-1（未~R~B~R~，旋转+染色）

<img src="image\dsa37.png" alt="image-20221018154354618" style="zoom:80%;" />

##### RR-2（已~R~B~R~，染色上溢）

<img src="image\dsa38.png" alt="image-20221018154446331" style="zoom:80%;" />

上溢会向上传播，至多 $\mathcal{O}(\log n)$

##### 复杂度

<img src="image\dsa39.png" alt="image-20221018154649694" style="zoom:80%;" />

计入查找关键码及接入操作，依旧不超过 $\mathcal{O}(\log n)$

旋转次数是常数次

只有RR-1会旋转，只有RR-2可能会上溢

#### 节点删除算法

被删除节点与其替代者同为黑色，那么在删除后，替代者子树黑高度减一，需要双黑修正（）

##### BB-1（~t~s-p-x，旋转+染色）

<img src="image\dsa40.png" alt="image-20221019132727182" style="zoom:80%;" />

##### BB-2-R（s-p(R)-x，染色）

<img src="image\dsa41.png" alt="image-20221019133331077" style="zoom:80%;" />

##### BB-2-B（s-p(B)-x，染色下溢）

<img src="image\dsa42.png" alt="image-20221019134448147" style="zoom:80%;" />

##### BB-3（~s~p-x，转-1或-2R）

<img src="image\dsa43.png" alt="image-20221019134743272" style="zoom:80%;" />

此时被删除节点x（或者说替代节点r）的兄弟s'必定是黑的，从而转化成先前讨论过的BB-1或BB-2-R

<img src="image\dsa44.png" alt="image-20221019135322268" style="zoom:80%;" />

### kd-树

<img src="image\dsa45.png" alt="image-20221019150927740" style="zoom:80%;" />

<img src="image\dsa46.png" alt="image-20221019150957162" style="zoom:80%;" />

### 搜索树比较

|         | 插入                                                         | 删除                                                         | 优点                                                         | 缺点                                                         |
| ------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| AVL树   | $\mathcal{O}(1)$                                             | $\mathcal{O}(\log n)$                                        | 无论查找、插入或者删除，最坏情况下时间复杂度为 $\mathcal{O}(\log n),\,\mathcal{O}(n)$ 的存储空间 | 额外封装平衡因子等变量；单次调整变化幅度可达 $\Omega(\log n)$ |
| Splay树 | 均摊 $\mathcal{O}(\log n)$                                   | 均摊 $\mathcal{O}(\log n)$                                   | 无需记录平衡因子，相比AVL树编程实现简单易行，分摊复杂度 $\mathcal{O}(\log n)$ 也与AVL树相当。利用局部性，缓存命中率高时，效率甚至会更高 | 不能杜绝单次最坏情况                                         |
| m阶B-树 | $\mathcal{O}(\log_m n)$                                      | $\mathcal{O}(\log_m n)$                                      | 针对外部查找，减少I/O次数；对外存的批量访问                  |                                                              |
| RB树    | $\mathcal{O}(\log n)$，至多 $\mathcal{O}(\log n)$ 次染色 $+\ 1$ 次“3+4”重构 | $\mathcal{O}(\log n)$，至多 $\mathcal{O}(\log n)$ 次染色 $+\ 1$ 次“3+4”重构 $+\ 1$ 次单旋 | 单次拓扑结构变化仅 $\mathcal{O}(1)$                          |                                                              |

###### **考过：红黑树与AVL树对比**

> 相比AVL树，红黑树每次插入删除操作所进行的旋转操作都是 $\mathcal{O}(1)$ 的，而AVL树的删除操作是 $\mathcal{O}(\log n)$ 的。因此红黑树的拓扑结构变化非常小，在持续性、历史版本的维护上具有优势，可以最大可能地节省空间。

## 词典

### 跳转表

相比于BBST，跳转表的算法更为简洁，且查询和维护操作在平均意义上仍均仅需 $\mathcal{O}(\log n)$ 时间

生长概率逐层减半，$S_k$ 中的每个关键码，在 $S_{k+1}$ 中出现的概率为 $p=\dfrac{1}{2}$，各塔的高度符合几何分布 $P\{h=k\}=p^{k-1}(1-p)$

故期望的塔高为 $Eh=\dfrac{1}{1-p}=2$

空间复杂度 $E(\sum_k|S_k|)<2n=\mathcal{O}(n)$，即为 $expected-\mathcal{O}(n)$

###### **考过：期望塔高**

在 $n$ 个节点的跳转表中，单个词条的期望塔高是 $\Theta(\log n)$

> 错误。单个词条期望塔高为 $2$。
>
> 注：横向跳转、纵向跳转都是 $expected-\mathcal{O}(\log n)$，总体时间复杂度为 $expected-\mathcal{O}(\log n)$

#### 查找

<img src="image\dsa47.png" alt="image-20221019160125524" style="zoom: 67%;" />

#### 插入/删除

注：先找到应当插入的位置，塔高是随机的（$p=\dfrac{1}{2}$ 概率增加一层）

<img src="image\dsa48.png" alt="image-20221019160423035" style="zoom:67%;" />

### 散列函数

<img src="image\dsa49.png" alt="image-20221019160738588" style="zoom:80%;" />

#### 除余法

$hash(key)=key\ mod\ M$

$M$ 需为素数，否则关键码被映射至 $[0,\,M)$ 的均匀度将大幅降低，发生冲突的概率随 $M$ 所含素因子的增多而迅速加大

$M$ 与词条关键码之间的最大公约数越大，发生冲突的可能性也将越大

除余法无论如何选择表长，都残留着某种连续性。比如，相邻关键码对应的散列地址总是彼此相邻，$0$ 总是被映射到 $0$

###### **考过：散列表M取值**

若M取不超过散列表长度的素数，则存储关键码不能保证其分布均匀

> 正确。
>
> 表长为7，M取7，则等于0~6的概率都是1/7
>
> 表长为7，M取5，则等于0~4的概率为1/5，冲突后才有可能分布到6、7，概率不同

#### MAD法

$hash(key)=(a\times key+b)\ mod\ M$，其中 $a>0,\,b>0,\,a\ mod\ M\neq 0$

依次乘（Multiply）、加（Add）、除（Divide）

具有更高阶的均匀性，除余法可以看做是MAD法的一个特例

### 冲突及其排解

<img src="image\dsa51.png" alt="image-20221019162200474" style="zoom:80%;" />

<img src="image\dsa50.png" alt="image-20221019162112417" style="zoom:80%;" />

#### 开散列策略

##### 多槽位

<img src="image\dsa52.png" alt="image-20221020131904324" style="zoom:80%;" />

装填因子降低至 $\lambda^{'}=\lambda/k$

极端情况下，单个同单元发生大量冲突，其余的桶都空闲的情况下，有一个桶却可能仍会溢出

##### 独立链

<img src="image\dsa53.png" alt="image-20221020132221750" style="zoom:80%;" />

查找过程中若发生冲突，则需要遍历整个列表

空间未必不连续分布，系统缓存几乎失效

##### 公共溢出区

<img src="image\dsa54.png" alt="image-20221020132554615" style="zoom:80%;" />

公共溢出区本身也是一个字典结构，相当于一种递归形式的散列表

在冲突不甚频繁的场合不失为一种好的选择

#### 闭散列策略

闭散列，散列地址空间对所有词条开放，故也称作开放定址；相应地，开散列也称作封闭定址

闭散列不得使用附加空间，装填因子需要适当降低，通常取 $\lambda\leqslant 0.5$

###### **常考：开散列和闭散列对比**

与开放散列相比，封闭散列有什么优点？（2019,2020）

> 就地解决，不需要额外空间；地址空间连续，可以更好地利用缓存

##### 线性试探

<img src="image\dsa55.png" alt="image-20221020133642033" style="zoom:80%;" />

相互冲突的关键码必属于同一查找链，但反过来，同一查找链中的关键码却未必相互冲突。多组各自冲突的关键码所对应的查找链，有可能相互交织和重叠，从而进一步增加各组关键码的查找长度

各查找链在物理上保持一定的连贯性，具有良好的数据局部性，可以充分发挥系统缓存的作用，降低I/O负担

##### 懒惰删除

<img src="image\dsa56.png" alt="image-20221020135901476" style="zoom:80%;" />

删除词条时，若按常规方法将其清空，会导致查找链断裂，后继词条丢失

利用lazyRemoval对象标记该桶内的词条已被删除，可以解决以上问题（“插入新的词条后，不必清除懒惰删除标志”————查找时，跳过冲突的桶和带懒惰标记的桶；插入时，选择第一个空桶，无论是否带有懒惰标记————不过根据软件工程的规范，最好还是要删除的）

###### **考过：最多有几个懒惰删除的桶**

散列长2017，单平方探测，已经存入了1000个元素，至多有几个懒惰删除的单元

> 单平方探测可以插入 $\lceil\dfrac{M}{2}\rceil$，所以最多可以插入1009个元素（2017是素数，故访问的桶就是 $\dfrac{2017+1}{2}=1009$ 个），最多9个桶为懒惰删除状态。

##### 重散列

装填因子一般建议是保持在 $0.9$ 以下，当装填因子过高的时候，会调用rehash()算法进行扩容————将原词条集逐一取出再插入

不可以简单地memcpy()，否则会继承原有冲突，且查找链可能会断裂

##### 平方试探

线性试探会加剧关键码的聚集趋势，形成聚集区段

<img src="image\dsa57.png" alt="image-20221020144256914" style="zoom:80%;" />

若长度M是素数，且装填因子 $\lambda \leqslant 0.5$，则只要有空桶，就一定能找出。否则平方试探有可能总是抵达不了某个空桶

若长度M是素数，则任一关键码对应的查找链中，前 $\dfrac{M+1}{2}$ 个桶必然互异

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601293521616-008178df-ca70-450e-b806-1f1d26ee5505.png?x-oss-process=image%2Fresize%2Cw_806%2Climit_0)

双向平方试探，表长取作 $M=4K+3$，则必然可以保证查找链前 $M$ 项均互异（即取遍整个散列表，装填因子达到 $1$ 之前，插入操作必然成功）

但若取作 $M=4K+1$，则不行————自然数 $n$ 可以表示为一对整数的平方和，当且仅当它的每一个 $M=4K+3$ 类的素因子均为偶数次方

###### **考过：4K+3**

长度 $M=4K+3$ 素数的散列表双平方探测一定能访问其全部元素

> 正确。

##### 再散列

选取二级散列函数，一旦发现冲突，则以 hash~2~(key)为偏移量继续尝试：

$[hash(key)+1\times hash_2(key)]\%M$

$[hash(key)+2\times hash_2(key)]\%M$

$[hash(key)+3\times hash_2(key)]\%M$

###### **考过：双散列函数**

<img src="image\dsa64.png" alt="image-20221022135558725" style="zoom:80%;" />

$31\%13=5$，$(5+8)\%13=0$，$(5+2\times8)\%13=8$

$36\%13=10$，$(10+3)\%13=0$，$(10+2\times3)\%13=3$，$(10+3\times3)\%13=6$，$(10+4\times3)\%13=9$

### 桶排序

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601295545509-32359e5f-8b16-42c2-a413-df47033c9641.png?x-oss-process=image%2Fresize%2Cw_788%2Climit_0)

若允许输入整数重复，又该如何高效地实现排序？

可以使用独立链法排解冲突。且在最后遍历独立链并串联起来的时候，留意链表的方向，甚至可以实现稳定排序

<img src="image\dsa58.png" alt="image-20221020153403958" style="zoom:80%;" />

### 最大间隙

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601295829969-3783f87a-7081-4223-8f5a-659c6bafdf96.png?x-oss-process=image%2Fresize%2Cw_802%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601295843017-31d82469-5ddf-49b4-9efa-e47e34775a74.png?x-oss-process=image%2Fresize%2Cw_815%2Climit_0)

散列函数选用 $hash(x)=\lfloor(n-1)*(x-lo)/(hi-lo)\rfloor$，把各点映射到各不相同的桶中

？？？平凡算法怎么就不能 $\mathcal{O}(n)$ 了？？？

### 基数排序

低位字段优先策略

<img src="image\dsa59.png" alt="image-20221020154459123" style="zoom:80%;" />

关键码可视为由个位、十位、百位三个组成，分别做一次桶排序

总运行时间不超过 $\mathcal{O}(n+M_1)+\mathcal{O}(n+M_2)+\cdots+\mathcal{O}(n+M_t)=\mathcal{O}(t\times(n+M))$

###### **常考：基数排序稳定性**

基数排序的底层排序算法不稳定，则得出来的结果不一定正确。（2018,2019,2020）

> 正确。如果不稳定，整体排序结果就有可能不正确了。
>
> 如 $V=\{19,\,17,\,23\}$，先个位，再十位。
>
> 第一步排序结果 $V^{'}=\{23,\,17,\,19\}$
>
> 若第二步排序不稳定，则可能会排成 $V^{''}=\{19,\,17,\,23\}$

## 优先级队列

### 堆

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601653652220-d2930e9e-4f6b-4129-abf1-93abaa8593b3.png?x-oss-process=image%2Fresize%2Cw_830%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601692336249-d02937ad-7cb9-4e0f-bb79-b220e591fc1c.png?x-oss-process=image%2Fresize%2Cw_836%2Climit_0)

向量下标：

若有左孩子 $2\cdot i(v)+1$

若有右孩子 $2\cdot i(v)+2$

若有父节点 $\lfloor(i(v)-1)/2\rfloor=\lceil(i(v)/2)-1\rceil$

最大内部节点 $\lfloor\dfrac{n-2}{2}\rfloor=\lfloor\dfrac{n}{2}\rfloor-1=\lceil\dfrac{n-3}{2}\rceil$

#### 堆序性

堆顶即最大元素（排序默认升序，但堆默认大顶堆）————堆顶以外的每个节点都不大于其父节点

###### **常考：堆操作的效率**

输入随机情况下完全二叉树的插入平均时间是常数

> 正确。插入的平均情况为 $\mathcal{O}(1)$，删除的平均情况依旧是 $\mathcal{O}(\log n)$

在理想随机情况下，完全二叉堆插入操作平均只需 $\mathcal{O}(1)$，尽管最坏情况下为 $\mathcal{O}(\log n)$

> 正确。插入平均 $\mathcal{O}(1)$，最坏 $\mathcal{O}(\log n)$，删除平均 $\mathcal{O}(\log n)$，最坏 $\mathcal{O}(\log n)$

### 插入上滤

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601693319462-b37fb924-13a5-49d8-8ae3-83d5e5d3051b.png?x-oss-process=image%2Fresize%2Cw_805%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601694265000-3908a041-bcd7-410d-addb-e8d65461154e.png?x-oss-process=image%2Fresize%2Cw_793%2Climit_0)

### 删除下滤

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601694942194-ee4ddf14-61c5-4d51-86dd-fd91c8ee985b.png?x-oss-process=image%2Fresize%2Cw_802%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601695558622-bbee0aaf-eed3-42d1-aa80-71d96ea83b59.png?x-oss-process=image%2Fresize%2Cw_818%2Climit_0)

### 建堆

#### 自上而下的上滤

从最上面的元素开始，一步一步插入新的节点以完成建堆，故为自上而下

![FastStoneEditor1.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601696429085-4b89867e-e53a-48e0-b840-b3ba9f44485c.png?x-oss-process=image%2Fresize%2Cw_788%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601696908153-7968c24f-c94e-4e5c-ab9c-386e37a6050b.png?x-oss-process=image%2Fresize%2Cw_793%2Climit_0)

#### 自下而上的下滤（Floyd）

从最下面的子树开始维护，一步一步将最大的节点提上来，故为自下而上

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1601698667310-e2f96f7a-ba41-407b-ae95-db763236f65a.png?x-oss-process=image%2Fresize%2Cw_676%2Climit_0" alt="image.png" style="zoom:120%;" />

<img src="image\dsa60.png" alt="image-20221021143654224" style="zoom:80%;" />

#### 效率

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601699680652-b14d5d9e-82cd-461b-a0c7-2e7475c399bb.png?x-oss-process=image%2Fresize%2Cw_806%2Climit_0)

###### **考过：建堆时间复杂度**

建立一个完全二叉堆需要 $\mathcal{O}(n\log n)$

> 错误。Floyd只要 $\mathcal{O}(n)$

### 堆排序

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601703242497-45250608-c5e3-430b-94cd-3d46f1c20649.png?x-oss-process=image%2Fresize%2Cw_815%2Climit_0)

#### 就地堆排序实例

![image-20221021154322748](image\dsa61.png)

![image-20221021154350526](image\dsa62.png)

### 锦标赛树

#### 胜者树

叶结点：待排序元素(选手)

内部结点：孩子中的胜者

**建堆** $\mathcal{O}(n)$

**删除**，**插入** $\mathcal{O}(\log n)$

**树根是全局冠军**。若约定小者为胜，则类似于小顶堆

**比赛过程**：从叶结点的两个兄弟结点之间相互比较，选出胜者其编号放入其父结点，父结点之间再参加下一轮比赛，直至根节点选出最终胜者

锦标赛排序空间复杂度 $\mathcal{O}(n)$，时间复杂度 $\mathcal{O}(n\log n)$

借助锦标赛树，从 $n$ 个元素中找出最小的 $k$ 个，就常系数而言，比小顶堆更优————Floyd算法的下滤在每一层都需要 $2$ 次比较，累计 $2*\log n$ 次，而锦标赛树找出最小的 $k$ 个只需要 $\mathcal{O}(k*\log n)$，每找出一个只需要 $\mathcal{O}(\log n)$

#### 败者树

叶结点：待排序元素(选手)

内部结点：孩子中的败者

在败者树中，用父结点记录其左右子结点进行比赛的败者，而让胜者参加下一轮的比赛

败者树的**根结点**记录的是**败者**（不一定是亚军），再**加一个结点**来记录整个比赛的胜利者

败者树的和胜者树真正的区别不在于建堆，而在于重赛(重构)过程，也即叶结点换了新的结点，要进行新一轮的比赛的时候————此时的败者树从叶结点开始，**不需要和兄弟结点比较**，选出两者中的败者，而是**和其原就记录为失败者的父结点比较**，以此顺着其祖先结点往上比较。避免了像胜者树那样交替访问沿途结点及其兄弟

#### 比较

堆是从上往下维护（下滤），每层比较两次，但不一定需要维护到底层；而锦标赛树则是从下往上维护，每层比较一次，但每次都需要上升到顶层，且空间是堆的两倍

重赛过程中，败者树只与其父结点比较，不用交替访问沿途结点及其兄弟，减少了访存时间

与败者树相比，胜者树在重赛过程中需反复将节点与其兄弟进行比较

败者树插入，删除操作的时间复杂度在常系数上优于胜者树

>  败者树效率 > 胜者树 > 堆

###### **常考：败者树与胜者树对比**

相比锦标赛树，败者树的优势是什么（2019,2020）

> 败者树每次比较只需要和父节点相比，且在与父节点比较失败时才需要写入，而锦标赛树则需要和兄弟节点比较并写入父节点。时间复杂度在常系数上有所优化。

### 左式堆

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601783460825-bb47cb48-18d4-44fb-bd8e-310b900286ac.png?x-oss-process=image%2Fresize%2Cw_800%2Climit_0)

#### 空节点路径长度

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601783766324-7150d329-344b-4578-a555-0c303b7fa9ea.png?x-oss-process=image%2Fresize%2Cw_790%2Climit_0)

#### 左倾性

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601784806061-1737637f-37ab-4a7c-8e66-dbeec6c447b4.png?x-oss-process=image%2Fresize%2Cw_812%2Climit_0)

左式堆中任一内节点x都应满足 $npl(x)=1+npl(rc(x))$，每个节点的 $npl$ 仅取决于其右孩子

但并不意味着左孩子的高度一定不小于右孩子

#### 最右侧通路

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601785555550-68e7b7a2-d912-4cc4-8734-fbb68d3d9d5a.png?x-oss-process=image%2Fresize%2Cw_767%2Climit_0)

尽管右孩子高度有可能大于左孩子，但由“$npl$ 仅取决于其右孩子”这一事实不难发现，每个节点的 $npl$ 值恰好等于其右侧通路的长度

###### **考过：右侧链长度与节点总数**

右侧路径长度为k的左式堆，其顶点数量至少（至多）为？

> 至少（刚刚满二叉树）$2^0+2^1+\cdots+2^k=2^{k+1}-1$，至多是无上限的
>
> 注1：上图PPT中节点数打错了
>
> 注2：上图右侧链最后一个是外部节点（可能是本来就有的，也可能是为了补成真二叉树而加上的），它的 $npl$ 是 $0$，故右侧链长度为 $k$，$npl(r)=k$，右侧链共有 $k+1$ 个节点。

#### 合并算法

左式堆分别以 $a$ 和 $b$ 为堆顶，不是一般性地 $a\geqslant b$，则递归地将 $a$ 的右子堆 $a_R$ 与堆 $b$ 合并。递归回溯后如有必要（比较 $npl$），还有可能将 $a_L$ 和合并后的 $a_R$ 交换位置

![image-20221021160432840](image\dsa63.png)

这里 (i) 也是一个“右孩子高度大于左孩子”的例子。实际是否要交换，需要看的是 $npl$ 而不是树高（$npl$ 即最右侧通路的长度）

###### **考过：合并过程**

Crane算法合并A、B两个左式堆为H，则H右侧链节点未必都来自A或者B的右侧链

> 正确。递归合并子树的时候，可能会在某个子树处交换左右子树。

#### 插入和删除

插入：将待插入节点视为一个只有根节点的树，合并即可 $\mathcal{O}(\log n)$

删除：删除根节点后将两棵子树合并即可，复杂度 $\mathcal{O}(\log n)$

## 串

### KMP算法

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1601864380018-37e5b345-2276-4a61-87f1-2d42fe3d19d7.png?x-oss-process=image%2Fresize%2Cw_808%2Climit_0)

最长自匹配=快速右移+避免回退：选取最大的t，必然最安全

```cpp
int* buildNext ( char* P ) {
    size_t m = strlen ( P ), j = 0;
    int* N = new int[m];
    int t = N[0] = -1;
    while ( j < m - 1 )
        if ( 0 > t || P[j] == P[t] ) {
            j ++; t ++;
            N[j] = ( P[j] != P[t] ? t : N[t] );
        } else
            t = N[t];
    return N;
}
```



KMP改进前：比较`P[ j ]`和`P[ t ]`

* 若匹配，`j`和`t`均加一，`Next[ j ]`赋为`t`；
* 若不匹配，`t`减小至`Next[ t ]`

KMP改进后：比较`P[ j ]`和`P[ t ]`

* 若匹配，`j`和`t`均加一，随后再比较新的`P[ j ]`和`P[ t ]`
  * 若匹配，`Next[ j ]`赋为`Next[ t ]`（下一次循环还会有同样的一次比较，也将匹配）；
  * 若不匹配，`Next[ j ]`赋为`t`（下一次循环还会有同样的一次比较，也将失配）;

* 若不匹配，`t`减小至`Next[ t ]`

<img src="image\dsa65.png" alt="image-20221023141507842" style="zoom:50%;" />

###### **常考：KMP算法过程**

<img src="image\dsa76.png" alt="image-20221025151502301" style="zoom:80%;" />

### BM算法

从右往左比对

#### 坏字符策略BC

构造BC[]表：每个字符都取最靠右者

<img src="image\dsa66.png" alt="image-20221023150914023" style="zoom:80%;" />

匹配过程：以(c1)为例，模式串“常”和主串“非”失配，查找BC表中的`BC["非"] = 2`，于是将模式串`P[2]`（即*最右边的那个“非”*）和主串的“非”对齐

<img src="image\dsa67.png" alt="image-20221023151035471" style="zoom:80%;" />

#### 好后缀策略GS

MS[]表：MS[j]表示在模式串P的位置j为结尾即P[0,j]的所有后缀中，与模式串的某一个后缀能匹配的最长者。MS[j]是一个字符串

ss[]表：ss[j] = MS[j]的长度

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602059402973-abf0ccce-ec15-47ba-8c24-b201353af90c.png?x-oss-process=image%2Fresize%2Cw_811%2Climit_0)

gs[]表：gs[j]表示在用BM_GS算法中在位置j失配时，此时模式串应当移动的位移量。gs[]表要从ss[]表构造出来

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602059843291-3d213298-126b-4808-b48f-a581c0907c9d.png?x-oss-process=image%2Fresize%2Cw_835%2Climit_0)

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602059856743-2403897c-5580-4eac-9d02-9ed3e3976ff8.png?x-oss-process=image%2Fresize%2Cw_820%2Climit_0)

构造SS[]表：从该字符开始往前`k`个字符，与最末尾的`k`个字符匹配

<img src="image\dsa68.png" alt="image-20221023151743952" style="zoom:80%;" />

构造GS[]表：失配后右移k个字符，后缀能覆盖。以`gs[10]=6`为例，右移6位后，原来rank=5处的R移到P后面，P的后缀“RICE”被完美覆盖

<img src="image\dsa69.png" alt="image-20221023154455021" style="zoom:80%;" />

匹配过程：失配后根据gs[]表右移相应位数

<img src="image\dsa70.png" alt="image-20221023154533214" style="zoom:80%;" />

###### **考过：BM_BC+GS算法过程**

对于长度为 $m$ 的随机 $0,\,1$ 串进行匹配，好后缀数组中 gs[0]==1 的概率为

> 如果只能移动一位，那只能是模式串每一位都相等才行

<img src="image\dsa77.png" alt="image-20221025153158015" style="zoom:80%;" />

### 算法纵览

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602062430281-9da4e760-e94c-4340-85d6-b37af8a50d57.png?x-oss-process=image%2Fresize%2Cw_736%2Climit_0)

|                    | 最好情况                                                     | 最坏情况                                                     | 平均情况                 |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------ |
| 蛮力算法（BF）     | 字符表较大，单次匹配概率较低时$\mathcal{O}(n+m)$             | 单次匹配概率较高时$\mathcal{O}(n\times m)$                   | $\mathcal{O}(n\times m)$ |
| KMP                | $\mathcal{O}(n+m)$                                           | $\mathcal{O}(n+m)$                                           | $\mathcal{O}(n+m)$       |
| 坏字符策略BM_BC    | 字符表较大，单次匹配概率较低时（末尾失配，直接*跳过一个模式串的长度*）$\mathcal{O}(n/m)$ | 单次匹配概率较高时（匹配到*最左边才失配*，然而又只能*一次移动一位*，退化成BF）$\mathcal{O}(n+m)$ |                          |
| 好后缀策略BM_BC+GS | 字符表较大，单次匹配概率较低时（末尾失配，直接*跳过一个模式串的长度*）$\mathcal{O}(n/m)$ | 即便是最坏情况，亦可像KMP一样保证单向地“滑动”模式串，而*不致回退*$\mathcal{O}(n+m)$ |                          |

###### **常考：各算法比较**

模式串和文本串均由 $26$ 个大写字母组成，那么蛮力算法在最好的情况下的时间复杂度（）KMP算法，在平均情况下蛮力算法的时间复杂度（）KMP算法

> 等于	等于
>
> $s$ 是字符集大小，在P和T的每一个对齐位置，需连续执行字符比对操作的期望次数不超过 $\dfrac{s}{s-1}\leqslant 2=\mathcal{O}(1)$
>
> 蛮力算法出现最好情况的概率是相当大的，而KMP复杂度则一直是稳定的 $\mathcal{O}(n+m)$

字符集各字符出现概率相同时，KMP算法渐进时间复杂度接近蛮力算法

> 正确。指的就是平均情况下的时间复杂度。

对没改进的next，KMP算法的时间复杂度是 $\mathcal{O}(n+m)$

> 正确。无论是否改进，永远都是 $\mathcal{O}(n+m)$

KMP算法相对于蛮力算法的优势在什么条件下足够明显？为什么？

> 单次匹配概率越大（字符集越小）的场合，KMP的优势就越明显；因为蛮力算法的时间复杂度随单次匹配成功率的增加而增加，KMP的时间复杂度不随单次匹配成功率的变化而变化。

### Karp-Rabin算法

字符串转化成散列码，称作其指纹。故字符串匹配可以转化为“判断主串T中是否有某个字串与模式串P拥有相同的指纹”的问题。

然而计算各子串的指纹十分耗时（随着字母表规模的增大，指纹计算，指纹比对都不能算是 $\mathcal{O}(1)$ 了，确切地说，整个算法的复杂度将退化成 $\mathcal{O}(n\times m)$ 的蛮力算法）

#### 散列压缩

加快指纹的比对速度：对散列空间进行压缩

若发生冲突（指纹相同）：则要逐位比对已确认是否真正匹配————故这种算法只有在匹配概率很小的情况下才有意义，大量冲突则会适得其反

#### 快速指纹更新

利用前后两子串的相关性，通过前一个子串计算出下一个子串的指纹，这样就能保证 $\mathcal{O}(1)$ 的复杂度

## 排序

### 快速排序

#### LUG

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1602998997522-0a424601-e6f2-4fb0-b06e-6a4d06084481.png" alt="image.png" style="zoom: 67%;" />

以第一个元素为轴点，随后将后续的序列划分为不大于轴点和不小于轴点的两部分

<img src="image\dsa71.png" alt="image-20221024143656707" style="zoom:80%;" />

##### 性能分析

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1602999376098-d1b341b4-21f1-42b0-9a57-be3fd78329e4.png?x-oss-process=image%2Fresize%2Cw_822%2Climit_0)

大量元素重复时，轴点位置总接近于 lo，递归深度达 $\mathcal{O}(n)$，时间复杂度达 $\mathcal{O}(n^2)$

改进：处理重复元素时，lo 和 hi 交替移动，使得轴点更接近与中点

#### LGU

<img src="https://cdn.nlark.com/yuque/0/2020/png/1458680/1603012315685-aaa42613-e09e-4bbd-b0ed-5c1a36c0019a.png" alt="image.png" style="zoom:67%;" />

这样仅有滚动后移的时候需要做一次交换，G拓展的情况不需要交换

### 选取与中位数

#### 众数

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603013463698-b8fbf4f8-67bc-4246-a6da-9cf0c1656269.png?x-oss-process=image%2Fresize%2Cw_815%2Climit_0)

（注意是“占一半以上的数”，而不是“最多的数”）

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603014067301-15323fbf-02cc-4c33-a36b-053159aa0a1d.png?x-oss-process=image%2Fresize%2Cw_826%2Climit_0)

#### 归并向量的中位数

<img src="image\dsa72.png" alt="image-20221024154536121" style="zoom:80%;" />

规模减半，中位数不变

* S~1~[mi~1~]<S~2~[mi~2~]，取S~1~右半段，S~2~左半段，（中位数在S~1~右半段或S~2~左半段中）
* S~1~[mi~1~]>S~2~[mi~2~]，取S~1~左半段，S~2~右半段，（中位数在S~1~左半段或S~2~右半段中）
* S~1~[mi~1~]=S~2~[mi~2~]，就是它

总体时间复杂度 $\mathcal{O}(\log n)$，精确地说，是 $\mathcal{O}(\log(\min(n_1,\,n_2)))$，子向量长度相等或接近时，此类问题的难度更大

#### 基于快速划分的选取

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1458680/1603017109106-49e6bf54-4143-4b74-b8f7-008a21bc22d4.png?x-oss-process=image%2Fresize%2Cw_820%2Climit_0)

尽管内循环仅需 $\mathcal{O}(hi-lo+1)$ 时间，但是很遗憾，外循环的次数却无法有效控制，与快速排序算法一样，最坏情况下外循环需执行 $\Omega(n)$ 次，总体运行时间为 $\mathcal{O}(n^2)$

#### k-选取算法

<img src="image\dsa74.png" alt="image-20221024160829494" style="zoom:80%;" />

<img src="image\dsa73.png" alt="image-20221024160514278" style="zoom:80%;" />

* |L|≥k，剪除子序列E和G
* |L|<k，但|L+E|≥k，必然就是E中的元素（E中元素都一样）
* |L+K|<k，剪除子序列L和E

<img src="image\dsa75.png" alt="image-20221024161709093" style="zoom:80%;" />

至少一半的子序列中，有半数的元素不小于M；至少一半的子序列中，有半数的元素不大于M，这意味着子序列L和G的规模均不会超过3n/4，所以递归需进一步处理的序列的规模绝不致超过原序列的3/4

$T(n)=cn+T(n/Q)+T(3n/4)=\mathcal{O}(n)$————顺便复习一下主定理，$cn=\Omega(n^{\log_{\frac{4}{3}}1})$，故复杂度为 $\theta(cn)$

可以在线性时间内完成k-选取

###### **考过：K-选取算法复杂度**

基于比较式算法可以在 $\mathcal{O}(n)$ 内确定无序数据的前 $10\%$

> 正确。使用k-选取算法排偏序，只要 $\mathcal{O}(n)$

### 希尔排序

如：分 $8,\,5,\,3,\,2,\,1$ 列分别排序

尽管该算法在最坏情况下需要运行 $\mathcal{O}(n^2)$ 时间，但随着向量的有序性不断提高（即逆序对的不断减少），运行时间将会锐减。具体地，当逆序元素的间距均不超过 $k$ 时，插入排序仅需 $\mathcal{O}(kn)$ 的运行时间。

#### 增量序列

首先考察 $\mathscr{H}_{shell}=\{1,\,2,\,4,\,8,\,\cdots,\,2^k,\,\cdots\}$，除首项之外其余各项均为偶数，因此在最后一步迭代之前，一直是奇数列和偶数列分别排序，直到最后一次迭代两个子列才会一起排序。

极端地，奇数列为 $\{8,\,9,\,10,\,11,\,12,\,13,\,14,\,15\}$，偶数列为 $\{0,\,1,\,2,\,3,\,4,\,5,\,6,\,7\}$，最后一轮排序仍然相当地无序，若使用插入排序，时间复杂度可达 $\mathcal{O}(n^2)$

按照“尽力避免增量值之间公共因子”的思路，有如下一种各项两两互素的增量序列 $\mathscr{H}_{ps}=\{1,\,3,\,7,\,15,\,31,\,63,\,\cdots,\,2^k-1,\,\cdots\}$，性能可以改进至 $\mathcal{O}(n^{3/2})$
